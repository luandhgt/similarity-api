# Multi-LLM Provider Support

HÆ°á»›ng dáº«n sá»­ dá»¥ng há»‡ thá»‘ng Multi-LLM Provider cho phÃ©p dá»… dÃ ng chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c chatbot khÃ¡c nhau (Claude, ChatGPT, Gemini, v.v.)

## Tá»•ng quan

Há»‡ thá»‘ng Multi-LLM Provider sá»­ dá»¥ng **Strategy Pattern** Ä‘á»ƒ cho phÃ©p chuyá»ƒn Ä‘á»•i linh hoáº¡t giá»¯a cÃ¡c nhÃ  cung cáº¥p AI khÃ¡c nhau mÃ  khÃ´ng cáº§n thay Ä‘á»•i code logic nghiá»‡p vá»¥.

### CÃ¡c Provider hiá»‡n táº¡i Ä‘Æ°á»£c há»— trá»£:

| Provider | Model máº·c Ä‘á»‹nh | Vision Support | API Key Required |
|----------|----------------|----------------|------------------|
| **Claude** (Anthropic) | claude-sonnet-4-5-20250929 | âœ… Yes | `CLAUDE_API_KEY` |
| **ChatGPT** (OpenAI) | gpt-4o | âœ… Yes | `OPENAI_API_KEY` |
| **Gemini** (Google) | gemini-pro | ğŸš§ Coming Soon | `GEMINI_API_KEY` |

---

## Cáº¥u hÃ¬nh

### 1. CÃ i Ä‘áº·t dependencies

ThÃªm vÃ o `requirements.txt`:

```txt
# Existing
anthropic>=0.18.0
aiohttp>=3.9.0

# New for OpenAI
openai>=2.0.0
```

CÃ i Ä‘áº·t:

```bash
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh trong file `.env`

Má»Ÿ file `.env.development` hoáº·c `.env.production` vÃ  cáº¥u hÃ¬nh:

```bash
# =============================================================================
# LLM PROVIDER SELECTION
# =============================================================================
# Choose one: "claude", "chatgpt", or "gemini"
LLM_PROVIDER=claude

# =============================================================================
# API KEYS
# =============================================================================
# Claude (Anthropic)
CLAUDE_API_KEY=sk-ant-api03-xxxxx

# OpenAI (ChatGPT)
OPENAI_API_KEY=sk-proj-xxxxx

# Google (Gemini)
GEMINI_API_KEY=your-gemini-key-here

# =============================================================================
# PROVIDER-SPECIFIC CONFIGURATION
# =============================================================================
# Claude Settings
CLAUDE_MODEL=claude-sonnet-4-5-20250929
CLAUDE_MAX_TOKENS=8000
CLAUDE_TEMPERATURE=0.7
CLAUDE_TIMEOUT=300

# OpenAI Settings
OPENAI_MODEL=gpt-4o
OPENAI_MAX_TOKENS=16384
OPENAI_TEMPERATURE=0.7
OPENAI_TIMEOUT=300

# Gemini Settings
GEMINI_MODEL=gemini-pro
GEMINI_MAX_TOKENS=4096
GEMINI_TEMPERATURE=0.7
GEMINI_TIMEOUT=300
```

---

## CÃ¡ch sá»­ dá»¥ng

### Chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c Provider

#### CÃ¡ch 1: Thay Ä‘á»•i trong file `.env`

```bash
# Sá»­ dá»¥ng Claude
LLM_PROVIDER=claude

# Sá»­ dá»¥ng ChatGPT
LLM_PROVIDER=chatgpt

# Sá»­ dá»¥ng Gemini (khi available)
LLM_PROVIDER=gemini
```

Sau Ä‘Ã³ restart server:

```bash
uvicorn main:app --reload
```

#### CÃ¡ch 2: Set environment variable khi cháº¡y

```bash
# Sá»­ dá»¥ng Claude
LLM_PROVIDER=claude uvicorn main:app --reload

# Sá»­ dá»¥ng ChatGPT
LLM_PROVIDER=chatgpt uvicorn main:app --reload
```

### Code Example - Sá»­ dá»¥ng trong Service

Provider Ä‘Æ°á»£c tá»± Ä‘á»™ng inject thÃ´ng qua ServiceContainer. Báº¡n khÃ´ng cáº§n thay Ä‘á»•i gÃ¬ trong code:

```python
from core.container import get_container, ServiceNames

# Get LLM provider (automatically selected based on config)
container = get_container()
llm_provider = container.get(ServiceNames.CLAUDE)

# Use provider (interface is same for all providers)
response = await llm_provider.generate_text(
    prompt="Analyze this event",
    system_prompt="You are an event analyst"
)

# Analyze image
result = await llm_provider.analyze_image(
    image_path="/path/to/image.jpg",
    prompt="Extract text from this image"
)

# Synthesize multiple images + texts
synthesized = await llm_provider.synthesize_with_images_and_texts(
    image_paths=["img1.jpg", "img2.jpg"],
    texts=["OCR text 1", "OCR text 2"],
    system_prompt="You are a content synthesizer",
    user_prompt="Create a comprehensive summary"
)
```

### Code Example - Direct Usage (Advanced)

Náº¿u báº¡n muá»‘n sá»­ dá»¥ng trá»±c tiáº¿p provider mÃ  khÃ´ng qua container:

```python
from services.llm_provider_factory import LLMProviderFactory
from config import Config

# Method 1: Using factory with config
provider_config = Config.get_llm_provider_config()
llm_provider = LLMProviderFactory.create_provider_from_config(provider_config)

# Method 2: Using factory directly
llm_provider = LLMProviderFactory.create_provider(
    provider_type="chatgpt",
    api_key="sk-proj-xxxxx",
    model="gpt-4o",
    max_tokens=4096
)

# Use provider
response = await llm_provider.generate_text("Hello, AI!")
```

---

## Architecture

### Class Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BaseLLMProvider       â”‚
â”‚   (Abstract Interface)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + generate_text()       â”‚
â”‚ + analyze_image()       â”‚
â”‚ + analyze_multiple()    â”‚
â”‚ + synthesize_content()  â”‚
â”‚ + synthesize_with_imgs()â”‚
â”‚ + get_provider_info()   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ implements
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude    â”‚  â”‚  ChatGPT     â”‚  â”‚   Gemini    â”‚
â”‚  Provider  â”‚  â”‚  Provider    â”‚  â”‚  Provider   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   (Coming Soon)
```

### Provider Factory Flow

```
1. Config.get_llm_provider_config()
   â†“
2. LLMProviderFactory.create_provider_from_config()
   â†“
3. Select provider class based on LLM_PROVIDER
   â†“
4. Create instance with API key and settings
   â†“
5. Return BaseLLMProvider instance
   â†“
6. Register in ServiceContainer
```

---

## API Reference

### BaseLLMProvider Interface

Táº¥t cáº£ providers Ä‘á»u implement cÃ¡c method sau:

#### `generate_text(prompt, system_prompt, max_tokens, temperature)`

Generate text tá»« text prompt Ä‘Æ¡n giáº£n.

**Parameters:**
- `prompt` (str): User prompt
- `system_prompt` (str, optional): System/context prompt
- `max_tokens` (int, optional): Max tokens to generate
- `temperature` (float, optional): Sampling temperature

**Returns:** `str` - Generated text

#### `analyze_image(image_path, prompt, system_prompt)`

PhÃ¢n tÃ­ch má»™t áº£nh vá»›i text prompt (OCR, extraction).

**Parameters:**
- `image_path` (str): Path to image file
- `prompt` (str): User prompt for analysis
- `system_prompt` (str, optional): System/context prompt

**Returns:** `str` - Analysis result text

#### `analyze_multiple_images(image_paths, prompts, system_prompt, parallel)`

PhÃ¢n tÃ­ch nhiá»u áº£nh (batch processing).

**Parameters:**
- `image_paths` (List[str]): List of image paths
- `prompts` (List[str]): List of prompts (one per image or one for all)
- `system_prompt` (str, optional): System prompt
- `parallel` (bool): Process in parallel or sequential

**Returns:** `List[Dict]` - List of results with success status

#### `synthesize_content(texts, system_prompt, user_prompt)`

Tá»•ng há»£p nhiá»u text thÃ nh ná»™i dung coherent.

**Parameters:**
- `texts` (List[str]): List of texts to synthesize
- `system_prompt` (str, optional): System prompt
- `user_prompt` (str, optional): User instructions

**Returns:** `str` - Synthesized content

#### `synthesize_with_images_and_texts(image_paths, texts, system_prompt, user_prompt)`

Tá»•ng há»£p cáº£ áº£nh vÃ  text (multimodal analysis).

**Parameters:**
- `image_paths` (List[str]): List of image paths
- `texts` (List[str]): List of texts (e.g., OCR results)
- `system_prompt` (str, optional): System prompt
- `user_prompt` (str, optional): User instructions

**Returns:** `str` - Synthesized multimodal content

#### `get_provider_info()`

Láº¥y thÃ´ng tin vá» provider.

**Returns:** `Dict` vá»›i keys:
- `provider`: Provider name
- `provider_type`: Enum value
- `model`: Model name
- `supports_vision`: Boolean
- `status`: "ready" or error status

---

## So sÃ¡nh Providers

### Claude (Anthropic)

**Æ¯u Ä‘iá»ƒm:**
- âœ… Context window lá»›n (200K tokens)
- âœ… Vision capabilities máº¡nh
- âœ… Accuracy cao trong phÃ¢n tÃ­ch phá»©c táº¡p
- âœ… Há»— trá»£ tá»‘t cho tiáº¿ng Viá»‡t

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Chi phÃ­ cao hÆ¡n GPT-4o
- âš ï¸ Rate limits nghiÃªm ngáº·t hÆ¡n

**Use cases tá»‘t nháº¥t:**
- PhÃ¢n tÃ­ch event phá»©c táº¡p
- OCR tiáº¿ng Viá»‡t
- Content synthesis dÃ i

### ChatGPT (OpenAI)

**Æ¯u Ä‘iá»ƒm:**
- âœ… GPT-4o cÃ³ vision tá»‘t
- âœ… Chi phÃ­ tháº¥p hÆ¡n Claude
- âœ… Rate limits thoÃ¡ng hÆ¡n
- âœ… Response nhanh

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Context window nhá» hÆ¡n (128K)
- âš ï¸ Accuracy tháº¥p hÆ¡n Claude má»™t chÃºt cho tasks phá»©c táº¡p

**Use cases tá»‘t nháº¥t:**
- High-throughput applications
- Cost-sensitive projects
- Quick prototyping

### Gemini (Google) - Coming Soon

**Æ¯u Ä‘iá»ƒm:**
- âœ… Miá»…n phÃ­ tier generous
- âœ… TÃ­ch há»£p tá»‘t vá»›i Google Cloud

**NhÆ°á»£c Ä‘iá»ƒm:**
- âš ï¸ Vision capabilities cÃ²n háº¡n cháº¿
- âš ï¸ ChÆ°a stable nhÆ° Claude/GPT

---

## Testing

### Test vá»›i nhiá»u providers

```bash
# Test vá»›i Claude
LLM_PROVIDER=claude pytest tests/

# Test vá»›i ChatGPT
LLM_PROVIDER=chatgpt pytest tests/

# Test táº¥t cáº£ providers
pytest tests/test_llm_providers.py
```

### Mock providers trong tests

```python
from unittest.mock import Mock
from services.llm_provider_base import BaseLLMProvider

# Create mock provider
mock_provider = Mock(spec=BaseLLMProvider)
mock_provider.generate_text.return_value = "Mocked response"

# Inject into container
container.register(ServiceNames.CLAUDE, mock_provider)
```

---

## Troubleshooting

### Provider initialization failed

**Error:** `âš ï¸ LLM provider: Provider initialization failed`

**Solution:**
1. Check API key trong `.env`:
   ```bash
   # For Claude
   echo $CLAUDE_API_KEY

   # For ChatGPT
   echo $OPENAI_API_KEY
   ```

2. Verify API key is valid:
   ```bash
   curl -H "Authorization: Bearer $OPENAI_API_KEY" \
        https://api.openai.com/v1/models
   ```

### Model does not support vision

**Error:** `NotImplementedError: Model gpt-3.5-turbo does not support vision`

**Solution:** Use vision-capable models:
- Claude: All Claude 3+ models support vision
- OpenAI: Use `gpt-4o`, `gpt-4-vision-preview`, or `gpt-4-turbo`

### Rate limit errors

**Error:** `429 Too Many Requests`

**Solution:**
1. Reduce concurrent requests in `.env`:
   ```bash
   MAX_CONCURRENT_REQUESTS=5
   ```

2. Add retry logic or switch to provider with higher limits

---

## Roadmap

### Current (v1.0)
- âœ… Claude Provider
- âœ… ChatGPT Provider
- âœ… Factory Pattern
- âœ… Configuration management

### Upcoming (v1.1)
- ğŸš§ Gemini Provider
- ğŸš§ Provider fallback mechanism
- ğŸš§ Cost tracking per provider
- ğŸš§ Response caching

### Future (v2.0)
- ğŸ“‹ Mistral AI Provider
- ğŸ“‹ Cohere Provider
- ğŸ“‹ Automatic provider selection based on task
- ğŸ“‹ A/B testing between providers

---

## Best Practices

### 1. Chá»n provider phÃ¹ há»£p vá»›i use case

```python
# For high-accuracy analysis
LLM_PROVIDER=claude

# For high-throughput, cost-effective
LLM_PROVIDER=chatgpt
```

### 2. Sá»­ dá»¥ng environment variables

```bash
# Development
LLM_PROVIDER=chatgpt  # Cheaper for testing

# Production
LLM_PROVIDER=claude   # Higher quality
```

### 3. Monitor costs

```python
provider_info = llm_provider.get_provider_info()
logger.info(f"Using provider: {provider_info['provider']}")
logger.info(f"Model: {provider_info['model']}")
```

### 4. Handle errors gracefully

```python
try:
    response = await llm_provider.generate_text(prompt)
except Exception as e:
    logger.error(f"LLM request failed: {e}")
    # Fallback to default response or retry with different provider
```

---

## Contributing

### Adding a new provider

1. Create provider class implementing `BaseLLMProvider`:
   ```python
   # services/gemini_provider.py
   from services.llm_provider_base import BaseLLMProvider

   class GeminiProvider(BaseLLMProvider):
       # Implement all abstract methods
       pass
   ```

2. Register in factory:
   ```python
   # services/llm_provider_factory.py
   _PROVIDER_CLASSES = {
       LLMProviderType.GEMINI: GeminiProvider,
   }
   ```

3. Add config in `config.py`:
   ```python
   GEMINI_API_KEY: str = os.getenv('GEMINI_API_KEY', '')
   GEMINI_MODEL: str = os.getenv('GEMINI_MODEL', 'gemini-pro')
   ```

4. Update tests and documentation

---

## Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check logs: `tail -f logs/image-similarity-api.log`
2. Verify config: `python -c "from config import Config; print(Config.get_llm_provider_config())"`
3. Test provider directly: `pytest tests/unit/test_llm_providers.py -v`

For issues, please create an issue on GitHub.
